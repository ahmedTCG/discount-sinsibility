#!/usr/bin/env python3
"""
01_prepare_data.py - Prepare raw CSV for model training.
"""

import argparse
import json
import os

import numpy as np
import pandas as pd

from config import (
    ID_COLS, DATE_COLS, LEAKAGE_COLS, DROP_COLS, WINDOW_TOKENS,
    COUNTRY_MIN_SHARE, TARGET_COLUMN, TARGET_SOURCE_COLS, DEFAULT_PATHS,
)


def parse_args():
    p = argparse.ArgumentParser()
    p.add_argument("--input", required=True, help="Path to raw CSV")
    p.add_argument("--out", default=DEFAULT_PATHS["data_clean"])
    p.add_argument("--metadata_out", default=DEFAULT_PATHS["metadata"])
    p.add_argument("--country_min_share", type=float, default=COUNTRY_MIN_SHARE)
    return p.parse_args()


def ensure_dir(path):
    d = os.path.dirname(path)
    if d:
        os.makedirs(d, exist_ok=True)


def main():
    args = parse_args()
    ensure_dir(args.out)
    ensure_dir(args.metadata_out)

    print(f"Loading CSV: {args.input}")
    df = pd.read_csv(args.input)
    print(f"Raw shape: {df.shape}")

    # Parse dates
    for c in DATE_COLS:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce")

    # Drop 100% null columns
    all_null_cols = [c for c in df.columns if df[c].isna().all()]
    if all_null_cols:
        print(f"Dropping all-null columns: {all_null_cols}")
        df = df.drop(columns=all_null_cols)

    # Fill rolling window NaNs
    window_cols = [c for c in df.columns if any(tok in c for tok in WINDOW_TOKENS) and c not in DATE_COLS and df[c].dtype != "object"]
    filled = 0
    for c in window_cols:
        n = int(df[c].isna().sum())
        if n:
            df[c] = df[c].fillna(0)
            filled += n
    print(f"Filled {filled:,} NaNs -> 0 across {len(window_cols)} window columns")

    if "avg_discount_per_order" in df.columns:
        n = int(df["avg_discount_per_order"].isna().sum())
        if n:
            df["avg_discount_per_order"] = df["avg_discount_per_order"].fillna(0)
            print(f"Filled {n:,} NaNs -> 0 in avg_discount_per_order")

    # Create target
    missing = [c for c in TARGET_SOURCE_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"Missing required columns for target: {missing}")

    df[TARGET_COLUMN] = ((df["share_of_orders_with_discount"] > 0) | (df["discount_abs_lifetime_eur"] > 0)).astype(np.int8)
    print("Target distribution:")
    print(df[TARGET_COLUMN].value_counts())
    print(f"Positive rate: {df[TARGET_COLUMN].mean():.4f}")

    # Drop leakage columns
    leakage_present = [c for c in LEAKAGE_COLS if c in df.columns]
    if leakage_present:
        df = df.drop(columns=leakage_present)
        print(f"Dropped leakage cols: {leakage_present}")

    # Country encoding
    if "country" not in df.columns:
        raise ValueError("Expected column 'country' not found")

    country_share = df["country"].value_counts(normalize=True)
    major_countries = country_share[country_share >= args.country_min_share].index.tolist()
    df["country_grouped"] = df["country"].where(df["country"].isin(major_countries), "OTHER")
    country_dummies = pd.get_dummies(df["country_grouped"], prefix="country", dummy_na=False)
    df = pd.concat([df.drop(columns=["country", "country_grouped"]), country_dummies], axis=1)
    print(f"Country kept (>= {args.country_min_share:.2%} share): {len(major_countries)}")

    # Drop unused columns
    for c in DROP_COLS:
        if c in df.columns:
            if df[c].nunique(dropna=False) <= 1:
                df = df.drop(columns=[c])
                print(f"Dropped constant column: {c}")
            else:
                df = df.drop(columns=[c])
                print(f"Dropped: {c}")

    drop_cols = [c for c in ID_COLS + DATE_COLS if c in df.columns]
    if drop_cols:
        df = df.drop(columns=drop_cols)
        print(f"Dropped ID/date columns: {drop_cols}")

    # Final cleanup
    nan_rows = int(df.isna().any(axis=1).sum())
    if nan_rows:
        print(f"Dropping {nan_rows:,} rows with remaining NaNs")
        df = df.dropna()

    obj_cols = df.select_dtypes(include=["object"]).columns.tolist()
    if obj_cols:
        raise ValueError(f"Non-numeric columns remain: {obj_cols}")

    print(f"Final shape: {df.shape}")

    df.to_parquet(args.out, index=False)
    print(f"Saved parquet -> {args.out}")

    metadata = {
        "country_min_share": args.country_min_share,
        "major_countries": major_countries,
        "feature_columns": [c for c in df.columns if c != TARGET_COLUMN],
        "target_column": TARGET_COLUMN,
        "notes": "Generated by 01_prepare_data.py",
    }
    with open(args.metadata_out, "w") as f:
        json.dump(metadata, f, indent=2)
    print(f"Saved metadata -> {args.metadata_out}")


if __name__ == "__main__":
    main()
